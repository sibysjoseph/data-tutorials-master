{"paragraphs":[{"text":"%md\n# Sentiment Analysis with Spark\nThis module will teach you how to build sentiment analysis algorithms with Apache Spark. We will be doing data transformation using Scala and Apache Spark 2, and we will be classifying tweets as happy or sad using a Gradient Boosting algorithm. Although we're focusing on sentiment analysis, Gradient Boosting is a versatile technique that can be applied to many classification problems. You should be able to reuse this code to classify text in many other ways, such as spam or not spam, news or not news, provided you can create enough labeled examples with which to train the model.","user":"anonymous","dateUpdated":"2018-07-31T22:08:22+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1533074888325_-1888343168","id":"20170314-235415_746413739","dateCreated":"2018-07-31T22:08:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:12901","dateFinished":"2018-07-31T22:08:22+0000","dateStarted":"2018-07-31T22:08:22+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Sentiment Analysis with Spark</h1>\n<p>This module will teach you how to build sentiment analysis algorithms with Apache Spark. We will be doing data transformation using Scala and Apache Spark 2, and we will be classifying tweets as happy or sad using a Gradient Boosting algorithm. Although we're focusing on sentiment analysis, Gradient Boosting is a versatile technique that can be applied to many classification problems. You should be able to reuse this code to classify text in many other ways, such as spam or not spam, news or not news, provided you can create enough labeled examples with which to train the model.</p>\n"}]}},{"text":"%md\n### Configuration\n\nBefore starting this model you should make sure HDFS and Spark2 are started and the shell interpreter is binded to this notebook. ","user":"anonymous","dateUpdated":"2018-07-31T22:08:24+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1533074888325_957166757","id":"20170316-002654_1627110001","dateCreated":"2018-07-31T22:08:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:12902","dateFinished":"2018-07-31T22:08:24+0000","dateStarted":"2018-07-31T22:08:24+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Configuration</h3>\n<p>Before starting this model you should make sure HDFS and Spark2 are started and the shell interpreter is binded to this notebook.</p>\n"}]}},{"text":"%md\n### Download Tweets pre-packaged tweets\n\nFor more information about the data and algorithms used visit the [accompanying Hortonworks tutorial.](https://hortonworks.com/tutorial/sentiment-analysis-with-apache-spark/#download-tweets)\n","user":"anonymous","dateUpdated":"2018-07-31T22:08:25+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1533074888326_-544473380","id":"20170315-185911_269819862","dateCreated":"2018-07-31T22:08:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:12903","dateFinished":"2018-07-31T22:08:25+0000","dateStarted":"2018-07-31T22:08:25+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Download Tweets pre-packaged tweets</h3>\n<p>For more information about the data and algorithms used visit the <a href=\"https://hortonworks.com/tutorial/sentiment-analysis-with-apache-spark/#download-tweets\">accompanying Hortonworks tutorial.</a></p>\n"}]}},{"text":"%sh\n\nmkdir /tmp/tweets\nrm -rf /tmp/tweets/*\ncd /tmp/tweets\nwget -O /tmp/tweets/tweets.zip https://raw.githubusercontent.com/hortonworks/data-tutorials/master/tutorials/hdp/sentiment-analysis-with-apache-spark/assets/tweets.zip\nunzip /tmp/tweets/tweets.zip\nrm /tmp/tweets/tweets.zip\n\n# Remove existing (if any) copy of data from HDFS. You could do this with Ambari file view.\nhdfs dfs -mkdir /tmp/tweets_staging/\nhdfs dfs -rm -r -f /tmp/tweets_staging/* -skipTrash\n\n# Move downloaded JSON file from local storage to HDFS\nhdfs dfs -put /tmp/tweets/* /tmp/tweets_staging\n\n","user":"anonymous","dateUpdated":"2018-07-31T22:08:08+0000","config":{"tableHide":false,"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1533074888326_-1556906714","id":"20170315-205558_857231102","dateCreated":"2018-07-31T22:08:08+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12904"},{"text":"%md\n### Load data into Spark\n\nLets load the tweets into Spark SQL and take a look at them.","user":"anonymous","dateUpdated":"2018-07-31T22:08:29+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1533074888326_1930174559","id":"20180727-000537_583840886","dateCreated":"2018-07-31T22:08:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:12905","dateFinished":"2018-07-31T22:08:29+0000","dateStarted":"2018-07-31T22:08:29+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Load data into Spark</h3>\n<p>Lets load the tweets into Spark SQL and take a look at them.</p>\n"}]}},{"text":"import org.apache.spark._\nimport org.apache.spark.rdd._\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.mllib.feature.HashingTF\nimport org.apache.spark.{SparkConf, SparkContext}\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.tree.GradientBoostedTrees\nimport org.apache.spark.mllib.tree.configuration.BoostingStrategy\nimport org.apache.spark._\nimport org.apache.spark.rdd._\nimport org.apache.spark.SparkContext._\nimport scala.util.{Success, Try}\n\n    val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n\n    var tweetDF = sqlContext.read.json(\"hdfs:///tmp/tweets_staging/*\")\n    tweetDF.show()\n","user":"anonymous","dateUpdated":"2018-07-31T22:08:08+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1533074888326_1666189207","id":"20170314-222922_910635069","dateCreated":"2018-07-31T22:08:08+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12906"},{"text":"%md\n## Clean Records\n\nIn the following paragrpahs we'll [clean-up](https://hortonworks.com/tutorial/sentiment-analysis-with-apache-spark/#clean-records) the data to prevent bias in the model.","user":"anonymous","dateUpdated":"2018-07-31T22:08:33+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1533074888326_1963660735","id":"20170315-214825_1815274191","dateCreated":"2018-07-31T22:08:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:12907","dateFinished":"2018-07-31T22:08:33+0000","dateStarted":"2018-07-31T22:08:33+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Clean Records</h2>\n<p>In the following paragrpahs we'll <a href=\"https://hortonworks.com/tutorial/sentiment-analysis-with-apache-spark/#clean-records\">clean-up</a> the data to prevent bias in the model.</p>\n"}]}},{"text":"\nvar messages = tweetDF.select(\"msg\")\nprintln(\"Total messages: \" + messages.count())\n\nvar happyMessages = messages.filter(messages(\"msg\").contains(\"happy\"))\nval countHappy = happyMessages.count()\nprintln(\"Number of happy messages: \" +  countHappy)\n\nvar unhappyMessages = messages.filter(messages(\"msg\").contains(\" sad\"))\nval countUnhappy = unhappyMessages.count()\nprintln(\"Unhappy Messages: \" + countUnhappy)\n\nval smallest = Math.min(countHappy, countUnhappy).toInt\n\n//Create a dataset with equal parts happy and unhappy messages\nvar tweets = happyMessages.limit(smallest).unionAll(unhappyMessages.limit(smallest))\n    \n    ","user":"anonymous","dateUpdated":"2018-07-31T22:08:08+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1533074888327_-1119410309","id":"20170314-222925_680225414","dateCreated":"2018-07-31T22:08:08+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12908"},{"text":"%md\n### Label the Data\n\nIn this paragraph we label the data and force our model to learn the difference between happy and sad.","user":"anonymous","dateUpdated":"2018-07-31T22:08:36+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1533074888327_-1619158317","id":"20170315-215800_466665578","dateCreated":"2018-07-31T22:08:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:12909","dateFinished":"2018-07-31T22:08:36+0000","dateStarted":"2018-07-31T22:08:36+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Label the Data</h3>\n<p>In this paragraph we label the data and force our model to learn the difference between happy and sad.</p>\n"}]}},{"text":"val messagesRDD = tweets.rdd\n//We use scala's Try to filter out tweets that couldn't be parsed\nval goodBadRecords = messagesRDD.map(\n  row =>{\n    Try{\n      val msg = row(0).toString.toLowerCase()\n      var isHappy:Int = 0\n      if(msg.contains(\" sad\")){\n        isHappy = 0\n      }else if(msg.contains(\"happy\")){\n        isHappy = 1\n      }\n      var msgSanitized = msg.replaceAll(\"happy\", \"\")\n      msgSanitized = msgSanitized.replaceAll(\"sad\",\"\")\n      //Return a tuple\n      (isHappy, msgSanitized.split(\" \").toSeq)\n    }\n  }\n)\n\n//We use this syntax to filter out exceptions\nval exceptions = goodBadRecords.filter(_.isFailure)\nprintln(\"total records with exceptions: \" + exceptions.count())\nexceptions.take(10).foreach(x => println(x.failed))\nvar labeledTweets = goodBadRecords.filter((_.isSuccess)).map(_.get)\nprintln(\"total records with successes: \" + labeledTweets.count())\n","user":"anonymous","dateUpdated":"2018-07-31T22:08:08+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1533074888327_1097927129","id":"20170314-234521_264925171","dateCreated":"2018-07-31T22:08:08+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12910"},{"text":"%md\nLet's take a look at our progress.","user":"anonymous","dateUpdated":"2018-07-31T22:08:39+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1533074888327_-1386785179","id":"20170315-220802_1466536053","dateCreated":"2018-07-31T22:08:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:12911","dateFinished":"2018-07-31T22:08:39+0000","dateStarted":"2018-07-31T22:08:39+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Let's take a look at our progress.</p>\n"}]}},{"text":"labeledTweets.take(10).foreach(x => println(x))\n","user":"anonymous","dateUpdated":"2018-07-31T22:08:08+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1533074888327_-1662742320","id":"20170315-221309_2084520780","dateCreated":"2018-07-31T22:08:08+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12912"},{"text":"%md\n### Transform Data\n\nGradient Boosting expects as input a vector (feature array) of fixed length, so we need a way to convert our tweets into some numeric vector that represents that tweet. To learn more about the algorithms used for the transpformation follow along in the [tutorial page](https://hortonworks.com/tutorial/sentiment-analysis-with-apache-spark/#transform-data).","user":"anonymous","dateUpdated":"2018-07-31T22:08:41+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false,"completionKey":"TAB"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1533074888327_-1295871847","id":"20170315-222108_1329422295","dateCreated":"2018-07-31T22:08:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:12913","dateFinished":"2018-07-31T22:08:41+0000","dateStarted":"2018-07-31T22:08:41+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Transform Data</h3>\n<p>Gradient Boosting expects as input a vector (feature array) of fixed length, so we need a way to convert our tweets into some numeric vector that represents that tweet. To learn more about the algorithms used for the transpformation follow along in the <a href=\"https://hortonworks.com/tutorial/sentiment-analysis-with-apache-spark/#transform-data\">tutorial page</a>.</p>\n"}]}},{"text":"    val hashingTF = new HashingTF(2000)\n\n    //Map the input strings to a tuple of labeled point + input text\n    val input_labeled = (labeledTweets.map(\n      t => (t._1, hashingTF.transform(t._2)))\n      .map(x => new LabeledPoint((x._1).toDouble, x._2)))","user":"anonymous","dateUpdated":"2018-07-31T22:08:08+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1533074888328_790125771","id":"20170315-221527_265576053","dateCreated":"2018-07-31T22:08:08+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12914"},{"text":"%md\nLet's take a look at how our vectors are hashed.","user":"anonymous","dateUpdated":"2018-07-31T22:08:44+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1533074888328_-1855302926","id":"20170315-225630_2078720791","dateCreated":"2018-07-31T22:08:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:12915","dateFinished":"2018-07-31T22:08:44+0000","dateStarted":"2018-07-31T22:08:44+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Let's take a look at how our vectors are hashed.</p>\n"}]}},{"text":"input_labeled.take(10).foreach(println)","user":"anonymous","dateUpdated":"2018-07-31T22:08:08+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1533074888328_1459299275","id":"20170315-225826_221402586","dateCreated":"2018-07-31T22:08:08+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12916"},{"text":"%md\nAs you can see, we've converted each tweet into a vector of integers. This will work great for a machine learning model, but we want to preserve some tweets in a form we can read.","user":"anonymous","dateUpdated":"2018-07-31T22:08:46+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1533074888328_-1840566589","id":"20170315-225921_1191286770","dateCreated":"2018-07-31T22:08:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:12917","dateFinished":"2018-07-31T22:08:46+0000","dateStarted":"2018-07-31T22:08:46+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>As you can see, we've converted each tweet into a vector of integers. This will work great for a machine learning model, but we want to preserve some tweets in a form we can read.</p>\n"}]}},{"text":"//We're keeping the raw text for inspection later\nvar sample = (labeledTweets.take(1000).map(\n  t => (t._1, hashingTF.transform(t._2), t._2))\n  .map(x => (new LabeledPoint((x._1).toDouble, x._2), x._3)))","user":"anonymous","dateUpdated":"2018-07-31T22:08:08+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1533074888328_-1544287643","id":"20170315-225625_1635512137","dateCreated":"2018-07-31T22:08:08+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12918"},{"text":"%md\n### Split into Training and Validation Sets\n\nWhen training any machine learning model you want to separate your data into a training set and a validation set. [Click here](https://hortonworks.com/tutorial/sentiment-analysis-with-apache-spark/#split-into-training-and-validation-sets) to find out why we do it and how.\n\n#### Fixing overfitting: \n\nIf you see that your validation accuracy is very low compared to your training accuracy, you can fix this overfitting by either increasing the size of your training data or by decreasing the number of parameters in your model.","user":"anonymous","dateUpdated":"2018-07-31T22:08:48+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1533074888329_536056888","id":"20170315-230331_1819827530","dateCreated":"2018-07-31T22:08:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:12919","dateFinished":"2018-07-31T22:08:49+0000","dateStarted":"2018-07-31T22:08:49+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Split into Training and Validation Sets</h3>\n<p>When training any machine learning model you want to separate your data into a training set and a validation set. <a href=\"https://hortonworks.com/tutorial/sentiment-analysis-with-apache-spark/#split-into-training-and-validation-sets\">Click here</a> to find out why we do it and how.</p>\n<h4>Fixing overfitting:</h4>\n<p>If you see that your validation accuracy is very low compared to your training accuracy, you can fix this overfitting by either increasing the size of your training data or by decreasing the number of parameters in your model.</p>\n"}]}},{"text":"\n    // Split the data into training and validation sets (30% held out for validation testing)\n    val splits = input_labeled.randomSplit(Array(0.7, 0.3))\n    val (trainingData, validationData) = (splits(0), splits(1))","user":"anonymous","dateUpdated":"2018-07-31T22:08:08+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1533074888329_-201322474","id":"20170315-220757_1010718454","dateCreated":"2018-07-31T22:08:08+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12920"},{"text":"%md\n### Build the Model\n\nThis machine learning application uses Gradient Boosting for classification.","user":"anonymous","dateUpdated":"2018-07-31T22:08:51+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1533074888329_-7438185","id":"20170315-233003_2105288198","dateCreated":"2018-07-31T22:08:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:12921","dateFinished":"2018-07-31T22:08:52+0000","dateStarted":"2018-07-31T22:08:52+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Build the Model</h3>\n<p>This machine learning application uses Gradient Boosting for classification.</p>\n"}]}},{"text":"\n    val boostingStrategy = BoostingStrategy.defaultParams(\"Classification\")\n    boostingStrategy.setNumIterations(20) //number of passes over our training data\n    boostingStrategy.treeStrategy.setNumClasses(2) //We have two output classes: happy and sad\n    boostingStrategy.treeStrategy.setMaxDepth(5) \n    //Depth of each tree. Higher numbers mean more parameters, which can cause overfitting.\n    //Lower numbers create a simpler model, which can be more accurate. \n    //In practice you have to tweak this number to find the best value.\n\n    val model = GradientBoostedTrees.train(trainingData, boostingStrategy)","user":"anonymous","dateUpdated":"2018-07-31T22:08:08+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1533074888329_1915356189","id":"20170315-232951_1402100499","dateCreated":"2018-07-31T22:08:08+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12922"},{"text":"%md\n### Evaluate Model\n\nLet's evaluate the model to see how it performed against our training and test set.","user":"anonymous","dateUpdated":"2018-07-31T22:08:55+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1533074888330_481191036","id":"20170315-234102_792111976","dateCreated":"2018-07-31T22:08:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:12923","dateFinished":"2018-07-31T22:08:55+0000","dateStarted":"2018-07-31T22:08:55+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Evaluate Model</h3>\n<p>Let's evaluate the model to see how it performed against our training and test set.</p>\n"}]}},{"text":"// Evaluate model on test instances and compute test error\nvar labelAndPredsTrain = trainingData.map { point =>\n  val prediction = model.predict(point.features)\n  Tuple2(point.label, prediction)\n}\n\nvar labelAndPredsValid = validationData.map { point =>\n  val prediction = model.predict(point.features)\n  Tuple2(point.label, prediction)\n}\n\n//Since Spark has done the heavy lifting already, lets pull the results back to the driver machine.\n//Calling collect() will bring the results to a single machine (the driver) and will convert it to a Scala array.\n\n//Start with the Training Set\nval results = labelAndPredsTrain.collect()\n\nvar happyTotal = 0\nvar unhappyTotal = 0\nvar happyCorrect = 0\nvar unhappyCorrect = 0\nresults.foreach(\n  r => {\n    if (r._1 == 1) {\n      happyTotal += 1\n    } else if (r._1 == 0) {\n      unhappyTotal += 1\n    }\n    if (r._1 == 1 && r._2 ==1) {\n      happyCorrect += 1\n    } else if (r._1 == 0 && r._2 == 0) {\n      unhappyCorrect += 1\n    }\n  }\n)\nprintln(\"unhappy messages in Training Set: \" + unhappyTotal + \" happy messages: \" + happyTotal)\nprintln(\"happy % correct: \" + happyCorrect.toDouble/happyTotal)\nprintln(\"unhappy % correct: \" + unhappyCorrect.toDouble/unhappyTotal)\n\nval testErr = labelAndPredsTrain.filter(r => r._1 != r._2).count.toDouble / trainingData.count()\nprintln(\"Test Error Training Set: \" + testErr)\n\n\n\n//Compute error for validation Set\nval results = labelAndPredsValid.collect()\n\nvar happyTotal = 0\nvar unhappyTotal = 0\nvar happyCorrect = 0\nvar unhappyCorrect = 0\nresults.foreach(\n  r => {\n    if (r._1 == 1) {\n      happyTotal += 1\n    } else if (r._1 == 0) {\n      unhappyTotal += 1\n    }\n    if (r._1 == 1 && r._2 ==1) {\n      happyCorrect += 1\n    } else if (r._1 == 0 && r._2 == 0) {\n      unhappyCorrect += 1\n    }\n  }\n)\nprintln(\"unhappy messages in Validation Set: \" + unhappyTotal + \" happy messages: \" + happyTotal)\nprintln(\"happy % correct: \" + happyCorrect.toDouble/happyTotal)\nprintln(\"unhappy % correct: \" + unhappyCorrect.toDouble/unhappyTotal)\n\nval testErr = labelAndPredsValid.filter(r => r._1 != r._2).count.toDouble / validationData.count()\nprintln(\"Test Error Validation Set: \" + testErr)\n\n","user":"anonymous","dateUpdated":"2018-07-31T22:08:08+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1533074888330_-224728252","id":"20170315-233951_1248887406","dateCreated":"2018-07-31T22:08:08+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12924"},{"text":"%md\nLets take some time to reflect on our result back at the [tutorials page.](https://hortonworks.com/tutorial/sentiment-analysis-with-apache-spark/#evaluate-model)","user":"anonymous","dateUpdated":"2018-07-31T22:08:59+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1533074888330_253982388","id":"20170320-190228_1141669795","dateCreated":"2018-07-31T22:08:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:12925","dateFinished":"2018-07-31T22:08:59+0000","dateStarted":"2018-07-31T22:08:59+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Lets take some time to reflect on our result back at the <a href=\"https://hortonworks.com/tutorial/sentiment-analysis-with-apache-spark/#evaluate-model\">tutorials page.</a></p>\n"}]}},{"text":"%md\n### Taking a closer look. \nLet's take some time to evaluate how our model did by disecting the data at the individual tweet level.","user":"anonymous","dateUpdated":"2018-07-31T22:09:08+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1533074888330_-243514852","id":"20180727-201640_1086570864","dateCreated":"2018-07-31T22:08:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:12926","dateFinished":"2018-07-31T22:09:08+0000","dateStarted":"2018-07-31T22:09:08+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Taking a closer look.</h3>\n<p>Let's take some time to evaluate how our model did by disecting the data at the individual tweet level.</p>\n"}]}},{"text":"//Print some examples and how they scored\nval predictions = sample.map { point =>\n  val prediction = model.predict(point._1.features)\n  (point._1.label, prediction, point._2)\n}\n\n//The first entry is the true label. 1 is happy, 0 is unhappy. \n//The second entry is the prediction.\npredictions.take(100).foreach(x => println(\"label: \" + x._1 + \" prediction: \" + x._2 + \" text: \" + x._3.mkString(\" \")))","user":"anonymous","dateUpdated":"2018-07-31T22:08:08+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1533074888330_-2135751474","id":"20170315-235355_1294226754","dateCreated":"2018-07-31T22:08:08+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12927"},{"text":"%md\nOnce you've trained your first model, you should go back and tweak the model parameters to see if you can increase model accuracy. In this case, try tweaking the depth of each tree and the number of iterations over the training data. You could also let the model see a greater percentage of happy tweets than unhappy tweets to see if that improves prediction accuracy for happy tweets.\n","user":"anonymous","dateUpdated":"2018-07-31T22:09:13+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1533074888331_-1491100476","id":"20170316-002506_341852523","dateCreated":"2018-07-31T22:08:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:12928","dateFinished":"2018-07-31T22:09:13+0000","dateStarted":"2018-07-31T22:09:13+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Once you've trained your first model, you should go back and tweak the model parameters to see if you can increase model accuracy. In this case, try tweaking the depth of each tree and the number of iterations over the training data. You could also let the model see a greater percentage of happy tweets than unhappy tweets to see if that improves prediction accuracy for happy tweets.</p>\n"}]}},{"text":"%md\n### Exporting the Model\n\nOnce your model is as accurate as you can make it, you can export it for production use. Models trained with Spark can be easily loaded back into a Spark Streaming workflow for use in production.","user":"anonymous","dateUpdated":"2018-07-31T22:09:16+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1533074888331_1869602255","id":"20170315-235739_1251499727","dateCreated":"2018-07-31T22:08:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:12929","dateFinished":"2018-07-31T22:09:16+0000","dateStarted":"2018-07-31T22:09:16+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Exporting the Model</h3>\n<p>Once your model is as accurate as you can make it, you can export it for production use. Models trained with Spark can be easily loaded back into a Spark Streaming workflow for use in production.</p>\n"}]}},{"text":"model.save(sc, \"hdfs:///tmp/tweets/RandomForestModel\")","user":"anonymous","dateUpdated":"2018-07-31T22:08:08+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1533074888331_957153487","id":"20170315-235824_1541028619","dateCreated":"2018-07-31T22:08:08+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12930"},{"text":"%md\nYou've now seen how to build a sentiment analysis model. The techniques you've seen here can be applied to other text classification models besides sentiment analysis. Try analyzing other keywords besides happy and sad and see what results you get. ","user":"anonymous","dateUpdated":"2018-07-31T22:09:23+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1533074888331_-786774232","id":"20170315-235646_797080385","dateCreated":"2018-07-31T22:08:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:12931","dateFinished":"2018-07-31T22:09:23+0000","dateStarted":"2018-07-31T22:09:23+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>You've now seen how to build a sentiment analysis model. The techniques you've seen here can be applied to other text classification models besides sentiment analysis. Try analyzing other keywords besides happy and sad and see what results you get.</p>\n"}]}},{"text":"\nprintln(model.predict(hashingTF.transform(\"To this cute little happy sunshine who never fails to bright up my day with his sweet lovely smiles \".split(\" \").toSeq)))","user":"anonymous","dateUpdated":"2018-07-31T22:08:08+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1533074888331_-921275094","id":"20170316-000532_1106625181","dateCreated":"2018-07-31T22:08:08+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12932"},{"user":"anonymous","dateUpdated":"2018-07-31T22:08:08+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1533074888332_1610895221","id":"20180718-000237_468407889","dateCreated":"2018-07-31T22:08:08+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12933"}],"name":"Sentiment Analysis Spark","id":"2DM5PJY2J","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"angular:shared_process":[],"sh:shared_process":[],"spark2:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}